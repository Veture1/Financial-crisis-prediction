{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a23f7c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyarrow.parquet as pq\n",
    "import pandas as pd\n",
    "import re\n",
    "from tqdm import tqdm  \n",
    "from transformers import BertTokenizer, pipeline\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3dfdb3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0,1'   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66b530c",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "578800ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             times Quarter_times  \\\n",
      "0       2006-10-20    2006-01-01   \n",
      "1       2006-10-21    2006-01-01   \n",
      "2       2006-10-23    2006-01-01   \n",
      "3       2006-10-23    2006-01-01   \n",
      "4       2006-10-24    2006-01-01   \n",
      "...            ...           ...   \n",
      "448390  2013-11-26    2013-01-01   \n",
      "448391  2013-11-26    2013-01-01   \n",
      "448392  2013-11-26    2013-01-01   \n",
      "448393  2013-11-26    2013-01-01   \n",
      "448394  2013-11-26    2013-01-01   \n",
      "\n",
      "                                                 articles  \n",
      "0       -- Inco's Net Soars on Higher Metal Prices, Br...  \n",
      "1       -- Jim Cramer: Diageo, Anheuser-Busch, Monster...  \n",
      "2       -- EU Energy Chief Backs German Plan for Price...  \n",
      "3       -- Ex-Plant Worker Shuster Pleads Guilty in Tr...  \n",
      "4       -- Jim Cramer: Bare Escentuals, Allergan, Medi...  \n",
      "...                                                   ...  \n",
      "448390  -- Rubber Drops to Two-Week Low After Forecast...  \n",
      "448391  -- SNB’s Jordan Sees No Reason to Remove Cap o...  \n",
      "448392  -- U.K. Lawmakers Seek ‘Sharp’ Change as Bank ...  \n",
      "448393  -- UBS Offers to Repurchase Some Shares of Pue...  \n",
      "448394  -- Hyundai Motor Unveils New Genesis to Boost ...  \n",
      "\n",
      "[448395 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "filepath = './Dataset/bloomberg_news_quarter.csv'\n",
    "df = pd.read_csv(filepath)\n",
    "df = df.drop(df.columns[0], axis=1)\n",
    "df.rename(columns={'0':'times', '1':'articles'}, inplace=True)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e544952",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_length = df['articles'].apply(lambda x: len(x) if pd.notnull(x) else 0)\n",
    "df['articles_len'] = txt_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea3f7fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['articles'].fillna('', inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0ff50735",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def Calculate_sentiment_score(text, max_length):\n",
    "    # Initializing the BERT tokenizer and sentiment analysis model\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "    sentiment_analysis = pipeline(\"sentiment-analysis\", model=\"ProsusAI/finbert\", batch_size=8, device=device)\n",
    "\n",
    "    # Split text\n",
    "    words = text.split()\n",
    "    subtexts = ['']\n",
    "    for word in words:\n",
    "        if len(tokenizer.encode(subtexts[-1] + word)) < max_length:\n",
    "            subtexts[-1] += word + ' '\n",
    "        else:\n",
    "            subtexts.append(word + ' ')\n",
    "\n",
    "    # Create a DataFrame to store the results\n",
    "    Sentiment_df = pd.DataFrame(columns=['subtext', 'label', 'score', 'length', 'weighted_score'])\n",
    "\n",
    "    # Perform sentiment analysis on each text segment\n",
    "    for subtext in subtexts:\n",
    "        if subtext.strip():  # Ignore empty text segments\n",
    "            result = sentiment_analysis(subtext)[0]\n",
    "            length = len(subtext.split())  # Calculate the length of the text segment (number of words)\n",
    "            weighted_score = result['score'] * length  # Calculate the weighted score\n",
    "            new_data = {'subtext': subtext, 'label': result['label'], 'score': result['score'], 'length': length, 'weighted_score': weighted_score}\n",
    "            new_df = pd.DataFrame([new_data])  # Create a DataFrame containing the new data\n",
    "            # Use the concat method to add the new DataFrame to the original DataFrame\n",
    "            df_list = [Sentiment_df, new_df]\n",
    "            Sentiment_df = pd.concat([df for df in df_list if not df.empty])\n",
    "\n",
    "    return Sentiment_df\n",
    "\n",
    "def AVG_Sentiment_score(df):\n",
    "    total_length = df['length'].sum()\n",
    "    total_score = 0\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        if row['label'] == 'negative':\n",
    "            total_score -= row['weighted_score']\n",
    "        elif row['label'] == 'positive':\n",
    "            total_score += row['weighted_score']\n",
    "        \n",
    "    weighted_average_score = total_score / total_length if total_length > 0 else 0\n",
    "    \n",
    "    return weighted_average_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9ce3d01f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前月份与上一个月份不同\n",
      "Saved\n",
      "                                             subtext     label     score  \\\n",
      "0  -- Inco's Net Soars on Higher Metal Prices, Br...  positive  0.792133   \n",
      "0  Vale. Inco's nickel production climbed to 125 ...  negative  0.960954   \n",
      "0  Chicago at dcrofts@bloomberg.net . To contact ...   neutral  0.931295   \n",
      "\n",
      "   length  weighted_score  \n",
      "0     313      247.937658  \n",
      "0     374      359.396688  \n",
      "0      16       14.900726  \n",
      "Time: 2006-10-20, Weighted Average Score: -0.15854769614478773\n",
      "                                             subtext    label     score  \\\n",
      "0  -- Jim Cramer: Diageo, Anheuser-Busch, Monster...  neutral  0.756490   \n",
      "0  trends are positive. He said to buy shares of ...  neutral  0.909033   \n",
      "\n",
      "   length  weighted_score  \n",
      "0     301      227.703631  \n",
      "0     154      139.991156  \n",
      "Time: 2006-10-21, Weighted Average Score: 0.0\n",
      "                                             subtext     label     score  \\\n",
      "0  -- EU Energy Chief Backs German Plan for Price...  negative  0.817719   \n",
      "0  Supply is stable and conditions aren't particu...  negative  0.475455   \n",
      "\n",
      "   length  weighted_score  \n",
      "0     319      260.852489  \n",
      "0     210       99.845535  \n",
      "Time: 2006-10-23, Weighted Average Score: -0.6818488150102655\n",
      "                                             subtext    label     score  \\\n",
      "0  -- Ex-Plant Worker Shuster Pleads Guilty in Tr...  neutral  0.546790   \n",
      "0  Assistant U.S. Attorney Benjamin Lawsky said i...  neutral  0.901644   \n",
      "\n",
      "   length  weighted_score  \n",
      "0     324      177.159845  \n",
      "0     117      105.492347  \n",
      "Time: 2006-10-23, Weighted Average Score: 0.0\n",
      "                                             subtext     label     score  \\\n",
      "0  -- Jim Cramer: Bare Escentuals, Allergan, Medi...  negative  0.883685   \n",
      "0  (SLB) has yet to hit its bottom, he said. He r...   neutral  0.935310   \n",
      "\n",
      "   length  weighted_score  \n",
      "0     286      252.733840  \n",
      "0     264      246.921747  \n",
      "Time: 2006-10-24, Weighted Average Score: -0.4595160722732544\n",
      "                                             subtext     label     score  \\\n",
      "0  -- Russia, Ukraine End Dispute That Cut Gas Su...  negative  0.813238   \n",
      "\n",
      "   length  weighted_score  \n",
      "0     283       230.14631  \n",
      "Time: 2006-10-24, Weighted Average Score: -0.8132378458976746\n",
      "                                             subtext     label     score  \\\n",
      "0  -- Huaneng Power's Third-Quarter Profit Rises ...  positive  0.948172   \n",
      "0  of Statistics. To contact the reporter on this...   neutral  0.944269   \n",
      "\n",
      "   length  weighted_score  \n",
      "0     327      310.052333  \n",
      "0      29       27.383791  \n",
      "Time: 2006-10-24, Weighted Average Score: 0.8709335188182552\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 21\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m article \u001b[38;5;129;01min\u001b[39;00m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124marticles\u001b[39m\u001b[38;5;124m'\u001b[39m]:        \n\u001b[0;32m     20\u001b[0m     max_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m500\u001b[39m\n\u001b[1;32m---> 21\u001b[0m     Sentiment_df \u001b[38;5;241m=\u001b[39m Calculate_sentiment_score(article, max_length)  \u001b[38;5;66;03m# 计算每篇文章的情感分数\u001b[39;00m\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28mprint\u001b[39m(Sentiment_df)\n\u001b[0;32m     23\u001b[0m     weighted_average_score \u001b[38;5;241m=\u001b[39m AVG_Sentiment_score(Sentiment_df)  \u001b[38;5;66;03m# 计算每篇文章的加权平均得分\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[18], line 7\u001b[0m, in \u001b[0;36mCalculate_sentiment_score\u001b[1;34m(text, max_length)\u001b[0m\n\u001b[0;32m      5\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m BertTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbert-base-uncased\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      6\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 7\u001b[0m sentiment_analysis \u001b[38;5;241m=\u001b[39m pipeline(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentiment-analysis\u001b[39m\u001b[38;5;124m\"\u001b[39m, model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProsusAI/finbert\u001b[39m\u001b[38;5;124m\"\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# 分割文本\u001b[39;00m\n\u001b[0;32m     10\u001b[0m words \u001b[38;5;241m=\u001b[39m text\u001b[38;5;241m.\u001b[39msplit()\n",
      "File \u001b[1;32mD:\\anaconda3\\Lib\\site-packages\\transformers\\pipelines\\__init__.py:870\u001b[0m, in \u001b[0;36mpipeline\u001b[1;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m framework \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    869\u001b[0m     model_classes \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m]}\n\u001b[1;32m--> 870\u001b[0m     framework, model \u001b[38;5;241m=\u001b[39m infer_framework_load_model(\n\u001b[0;32m    871\u001b[0m         model,\n\u001b[0;32m    872\u001b[0m         model_classes\u001b[38;5;241m=\u001b[39mmodel_classes,\n\u001b[0;32m    873\u001b[0m         config\u001b[38;5;241m=\u001b[39mconfig,\n\u001b[0;32m    874\u001b[0m         framework\u001b[38;5;241m=\u001b[39mframework,\n\u001b[0;32m    875\u001b[0m         task\u001b[38;5;241m=\u001b[39mtask,\n\u001b[0;32m    876\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs,\n\u001b[0;32m    877\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m    878\u001b[0m     )\n\u001b[0;32m    880\u001b[0m model_config \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\n\u001b[0;32m    881\u001b[0m hub_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_commit_hash\n",
      "File \u001b[1;32mD:\\anaconda3\\Lib\\site-packages\\transformers\\pipelines\\base.py:269\u001b[0m, in \u001b[0;36minfer_framework_load_model\u001b[1;34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[0m\n\u001b[0;32m    263\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[0;32m    264\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel might be a PyTorch model (ending with `.bin`) but PyTorch is not available. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    265\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrying to load the model with Tensorflow.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    266\u001b[0m     )\n\u001b[0;32m    268\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 269\u001b[0m     model \u001b[38;5;241m=\u001b[39m model_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(model, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    271\u001b[0m         model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[1;32mD:\\anaconda3\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:566\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    564\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    565\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[1;32m--> 566\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    567\u001b[0m         pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39mmodel_args, config\u001b[38;5;241m=\u001b[39mconfig, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    568\u001b[0m     )\n\u001b[0;32m    569\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    570\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    571\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    572\u001b[0m )\n",
      "File \u001b[1;32mD:\\anaconda3\\Lib\\site-packages\\transformers\\modeling_utils.py:3285\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   3282\u001b[0m     config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_check_and_enable_flash_attn_2(config, torch_dtype\u001b[38;5;241m=\u001b[39mtorch_dtype, device_map\u001b[38;5;241m=\u001b[39mdevice_map)\n\u001b[0;32m   3284\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ContextManagers(init_contexts):\n\u001b[1;32m-> 3285\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(config, \u001b[38;5;241m*\u001b[39mmodel_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[0;32m   3287\u001b[0m \u001b[38;5;66;03m# make sure we use the model's config since the __init__ call might have copied it\u001b[39;00m\n\u001b[0;32m   3288\u001b[0m config \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\n",
      "File \u001b[1;32mD:\\anaconda3\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1525\u001b[0m, in \u001b[0;36mBertForSequenceClassification.__init__\u001b[1;34m(self, config)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_labels \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mnum_labels\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig \u001b[38;5;241m=\u001b[39m config\n\u001b[1;32m-> 1525\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbert \u001b[38;5;241m=\u001b[39m BertModel(config)\n\u001b[0;32m   1526\u001b[0m classifier_dropout \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1527\u001b[0m     config\u001b[38;5;241m.\u001b[39mclassifier_dropout \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mclassifier_dropout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m config\u001b[38;5;241m.\u001b[39mhidden_dropout_prob\n\u001b[0;32m   1528\u001b[0m )\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mDropout(classifier_dropout)\n",
      "File \u001b[1;32mD:\\anaconda3\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:882\u001b[0m, in \u001b[0;36mBertModel.__init__\u001b[1;34m(self, config, add_pooling_layer)\u001b[0m\n\u001b[0;32m    879\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(config)\n\u001b[0;32m    880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig \u001b[38;5;241m=\u001b[39m config\n\u001b[1;32m--> 882\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings \u001b[38;5;241m=\u001b[39m BertEmbeddings(config)\n\u001b[0;32m    883\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder \u001b[38;5;241m=\u001b[39m BertEncoder(config)\n\u001b[0;32m    885\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;241m=\u001b[39m BertPooler(config) \u001b[38;5;28;01mif\u001b[39;00m add_pooling_layer \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mD:\\anaconda3\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:185\u001b[0m, in \u001b[0;36mBertEmbeddings.__init__\u001b[1;34m(self, config)\u001b[0m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, config):\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m--> 185\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mword_embeddings \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mEmbedding(config\u001b[38;5;241m.\u001b[39mvocab_size, config\u001b[38;5;241m.\u001b[39mhidden_size, padding_idx\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpad_token_id)\n\u001b[0;32m    186\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embeddings \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mEmbedding(config\u001b[38;5;241m.\u001b[39mmax_position_embeddings, config\u001b[38;5;241m.\u001b[39mhidden_size)\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken_type_embeddings \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mEmbedding(config\u001b[38;5;241m.\u001b[39mtype_vocab_size, config\u001b[38;5;241m.\u001b[39mhidden_size)\n",
      "File \u001b[1;32mD:\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\sparse.py:144\u001b[0m, in \u001b[0;36mEmbedding.__init__\u001b[1;34m(self, num_embeddings, embedding_dim, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse, _weight, _freeze, device, dtype)\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    142\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m=\u001b[39m Parameter(torch\u001b[38;5;241m.\u001b[39mempty((num_embeddings, embedding_dim), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfactory_kwargs),\n\u001b[0;32m    143\u001b[0m                             requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m _freeze)\n\u001b[1;32m--> 144\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset_parameters()\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    146\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_weight\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m [num_embeddings, embedding_dim], \\\n\u001b[0;32m    147\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mShape of weight does not match num_embeddings and embedding_dim\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[1;32mD:\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\sparse.py:153\u001b[0m, in \u001b[0;36mEmbedding.reset_parameters\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    152\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreset_parameters\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 153\u001b[0m     init\u001b[38;5;241m.\u001b[39mnormal_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight)\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fill_padding_idx_with_zero()\n",
      "File \u001b[1;32mD:\\anaconda3\\Lib\\site-packages\\torch\\nn\\init.py:155\u001b[0m, in \u001b[0;36mnormal_\u001b[1;34m(tensor, mean, std)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39moverrides\u001b[38;5;241m.\u001b[39mhas_torch_function_variadic(tensor):\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39moverrides\u001b[38;5;241m.\u001b[39mhandle_torch_function(normal_, (tensor,), tensor\u001b[38;5;241m=\u001b[39mtensor, mean\u001b[38;5;241m=\u001b[39mmean, std\u001b[38;5;241m=\u001b[39mstd)\n\u001b[1;32m--> 155\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _no_grad_normal_(tensor, mean, std)\n",
      "File \u001b[1;32mD:\\anaconda3\\Lib\\site-packages\\torch\\nn\\init.py:19\u001b[0m, in \u001b[0;36m_no_grad_normal_\u001b[1;34m(tensor, mean, std)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_no_grad_normal_\u001b[39m(tensor, mean, std):\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 19\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m tensor\u001b[38;5;241m.\u001b[39mnormal_(mean, std)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "Sentiment_Scores = []\n",
    "last_month = None\n",
    "\n",
    "for times, group in df.groupby('times'):\n",
    "    current_month = times[5:7]  # Get the current month\n",
    "    score_perday = []\n",
    "    if current_month != last_month:\n",
    "        print(\"The current month is different from the previous month\")\n",
    "        # Save sentiment scores to a local CSV file by month\n",
    "        file_name = f\"./FinBERT/sentiment_scores_{pd.to_datetime(times).strftime('%Y-%m-%d')}.csv\"\n",
    "        Sentiment_Scores_df = pd.DataFrame(Sentiment_Scores, columns=['sentiment_scores','times'])\n",
    "        Sentiment_Scores_df.to_csv(file_name)\n",
    "        print(\"Saved\")\n",
    "        \n",
    "        # Reset the Sentiment_Scores list to store scores for the next month\n",
    "        Sentiment_Scores = []\n",
    "        last_month = current_month  # Save the current month as the last month\n",
    "        \n",
    "    for article in group['articles']:        \n",
    "        max_length = 500\n",
    "        Sentiment_df = Calculate_sentiment_score(article, max_length)  # Calculate sentiment scores for each article\n",
    "        print(Sentiment_df)\n",
    "        weighted_average_score = AVG_Sentiment_score(Sentiment_df)  # Calculate the weighted average score for each article\n",
    "        score_perday.append(weighted_average_score) \n",
    "        print(f\"Time: {times}, Weighted Average Score: {weighted_average_score}\")\n",
    "    # Append the list of sentiment scores for a day\n",
    "    Sentiment_Scores.append([score_perday,times])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666ee931",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
